# -*- coding: utf-8 -*-
"""Analisis Sentimen Tweeter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TM2Yrc9cANqVs7RQYne0s9VAHXY-2lmK

# **Import Package**
"""

!pip install contractions

!pip install wordcloud

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')

import contractions
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
import re
import string
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import RegexpTokenizer

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import ComplementNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import metrics

"""# **Pre-Processing**"""

df = pd.read_csv('/content/drive/MyDrive/DBS/ML TERAPAN/Dataset - Train.csv')

df.head()

df.shape

df.isna().sum()

df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()

df.dropna(inplace = True)



df_clean = df.drop_duplicates()

df_clean['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()

def cleaning(text):
    # 1. Lowercase
    text = text.lower()

    # 2. Remove mention first before punctuation
    text = re.sub(r"@[\w_]+", "", text) # Remove mention

    # 3. Remove numbers
    text = re.sub(r"\b\d+\b", "", text)

    # 4. Remove URLs
    text = re.sub('https?://\S+|www\.\S+', '', text)

    # 5. Remove HTML tags
    text = re.sub('<.*?>+', '', text)

    # 6. Remove punctuation
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)

    # 7. Remove newlines and some extra characters
    text = re.sub('\n', '', text)
    text = re.sub('[’“”…]', '', text)


    # 8. Remove RT and mention literal (optional)
    text = re.sub(r'\brt\b', '', text)
    text = re.sub(r'\bmention\b', '', text)

    # 9. Remove emoji
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)


    # 10. Fix contractions
    text = contractions.fix(text)

    # 11. Remove stopwords
    stop_list = stopwords.words('english')
    text = ' '.join([word for word in text.split() if word not in stop_list])

    # 12. Lemmatization
    lemmatizer = WordNetLemmatizer()
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

df_clean['tweet_text'] = df_clean['tweet_text'].apply(cleaning)

df_clean.head()

"""# **Exploratory Data Analysis**"""

df_clean.info()

df_clean.shape

plt.figure(figsize=(10,5))
sns.countplot(x='is_there_an_emotion_directed_at_a_brand_or_product', data=df_clean, palette = {"Positive emotion": 'green', "Negative emotion": 'red', "No emotion toward brand or product": 'yellow', "I can't tell": "black"})
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.title('Distribution of Emotions')
plt.xticks(rotation=-30)
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(x='emotion_in_tweet_is_directed_at', data=df_clean)
plt.xlabel('Brand')
plt.ylabel('Count')
plt.title('Distribution of Brand')
plt.xticks(rotation=-90)
plt.show()

sentiment_counts = df_clean.groupby(['emotion_in_tweet_is_directed_at', 'is_there_an_emotion_directed_at_a_brand_or_product']).size().unstack(fill_value=0)

sentiment_counts.plot(kind='bar', stacked=False, figsize=(12, 6), color = {"Positive emotion": 'green', "Negative emotion": 'red', "No emotion toward brand or product": 'yellow', "I can't tell": "black"})
plt.title('Sentiment Distribution for Each Brand')
plt.xlabel('Brand')
plt.ylabel('Number of Tweets')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

stop_list = stopwords.words('english')

all_tweets = ' '.join(df_clean['tweet_text'])

wordcloud = WordCloud(width = 800, height = 800,
                      background_color ='white',
                      stopwords = set(stop_list),
                      min_font_size = 10).generate(all_tweets)

# plot the word cloud
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

"""# **Data Preparation**"""

df_clean['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()

df_clean['is_there_an_emotion_directed_at_a_brand_or_product'].replace({"Positive emotion": 1,
                                                                        "Negative emotion": 0,
                                                                        "No emotion toward brand or product": 2,
                                                                        "I can't tell": 2}, inplace=True)
df_clean.head()

df_clean.drop('emotion_in_tweet_is_directed_at', axis=1, inplace=True)

tfidf = TfidfVectorizer()
text_count = tfidf.fit_transform(df_clean['tweet_text'])

"""## Splitting data"""

X=text_count
y=df_clean['is_there_an_emotion_directed_at_a_brand_or_product']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=42, stratify=y)

"""## Fitting Data"""

cnb = ComplementNB()
cnb.fit(X_train, y_train)

param_dist = {
    'alpha': np.arange(0,1,0.1),
    'norm': [True, False]
}

scorer = make_scorer(f1_score, average='macro')

cnb2 = ComplementNB()

grid_search = GridSearchCV(
    estimator=cnb2,
    param_grid=param_dist,
    scoring=scorer,
    cv=5,
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

"""# Evaluation

## Default Parameter
"""

predicted = cnb.predict(X_test)
accuracy_score = metrics.accuracy_score(predicted, y_test)
print('ComplementNB model accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')
print('------------------------------------------------')
print('Confusion Matrix:')
print(pd.DataFrame(confusion_matrix(y_test, predicted)))
print('------------------------------------------------')
print('Classification Report:')
print(classification_report(y_test, predicted))

f1_score(y_test, predicted, average='macro')

y_test.value_counts()

519/(y_test.shape[0])

"""## GridSearchCV"""

print("Best Params:", grid_search.best_params_)
print("Best F1 Macro Score:", grid_search.best_score_)

predicted2 = grid_search.predict(X_test)
accuracy_score = metrics.accuracy_score(predicted2, y_test)
print('ComplementNB model accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')
print('------------------------------------------------')
print('Confusion Matrix:')
print(pd.DataFrame(confusion_matrix(y_test, predicted2)))
print('------------------------------------------------')
print('Classification Report:')
print(classification_report(y_test, predicted2))

"""# Inference"""

def inference(text):
  text = cleaning(text)
  text = tfidf.transform([text])
  prediction = cnb.predict(text)

  if prediction == 1:
    return 'Positive emotion'
  elif prediction == 0:
    return 'Negative emotion'
  else:
    return 'No emotion toward brand or product'

text1 = "Apple has a good technology"
text2 = "Ipad is very useless"

print(f"Text 1: {inference(text1)}\nText 2: {inference(text2)}")